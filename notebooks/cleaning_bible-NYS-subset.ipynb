{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a3926a-7728-49d0-9c0e-76b759396b8f",
   "metadata": {},
   "source": [
    "### this file is used to convert data from grib to nc. you are able to select the dates, vars, and prs levels you want and it will return training/testing features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f08a849-3a60-4c6b-8a37-678edfbe64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import timedelta\n",
    "import datetime\n",
    "import xarray as xr\n",
    "import re\n",
    "import argparse\n",
    "# import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import cartopy.crs as ccrs\n",
    "from pathlib import Path\n",
    "import calendar\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c2f1f21-baf5-4c1b-928a-a59bfe62a5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make list of good files and print out any bad files\n",
    "def is_non_zero_file(fpath):\n",
    "    if os.path.isfile(fpath) and os.path.getsize(fpath) > 0:\n",
    "#         print(\"this file is good \", fpath) \n",
    "        return fpath\n",
    "    else:\n",
    "        print(\"file not found or corrupt \", fpath) \n",
    "        return\n",
    "#         raise Exception('No good file found ', fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4e9261df-7c6b-4e5b-ab04-cee7f7a7c35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def days2files(input_path, start_date, end_date, init_time, model, fh, prs):\n",
    "    DATES = pd.date_range(start_date, end_date)\n",
    "                          \n",
    "    if model == 'nam':\n",
    "        fileList = [f'{input_path}{DATE:%Y}/{DATE:%m}/nam_218_{DATE:%Y%m%d}_{init_time}00_0{f:02d}.grb2' for DATE in DATES for f in fh]\n",
    "    elif model == 'hrrr':\n",
    "        if prs:\n",
    "            fileList = [f'{input_path}prs/{DATE:%Y}/{DATE:%m}/{DATE:%Y%m%d}_hrrr.t{init_time}z.wrfprsf{f:02d}.grib2' for DATE in DATES for f in fh]\n",
    "        else:            \n",
    "            fileList = [f'{input_path}/{DATE:%Y}/{DATE:%m}/{DATE:%Y%m%d}_hrrr.t{init_time}z.wrfsfcf{f:02d}.grib2' for DATE in DATES for f in fh]\n",
    "    elif model == 'gfs':\n",
    "        fileList = [f'{input_path}{DATE:%Y}/{DATE:%m}/gfs_4_{DATE:%Y%m%d}_{init_time}00_0{f:02d}.grb2' for DATE in DATES for f in fh]\n",
    "    \n",
    "    fileList.sort()\n",
    "    \n",
    "    if len(fileList) == 0:\n",
    "        raise Exception('No files found')\n",
    "    \n",
    "    goodFiles = []\n",
    "    \n",
    "    #check to make sure file is not empty and actually is there\n",
    "    for file in fileList:\n",
    "        if (is_non_zero_file(file)) is not None:\n",
    "            goodFiles.append(is_non_zero_file(file))\n",
    "        \n",
    "    print(goodFiles)\n",
    "    return goodFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d76461a9-df5c-4ada-a4e0-384d61f33398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#call this preprocessing when reading in multi files to clean up and only take what we need while reading to save time and mem\n",
    "def preprocessNam(ds):\n",
    "    #get available vars\n",
    "    available_vars = [ v for v in nam_vars if v in ds.keys() ]\n",
    "    #make sure full grid is there\n",
    "    if ds.dims['x'] != 614 and ds.dims['y'] != 428:\n",
    "        print(\"bad dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    ds = ds[available_vars]\n",
    "    return ds\n",
    "\n",
    "def preprocessNamALL(ds):\n",
    "    #make sure full grid is there\n",
    "    if ds.dims['x'] != 614 and ds.dims['y'] != 428:\n",
    "        print(\"bad dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cd055e6f-f3a4-4c2a-ab4a-33af22040207",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#call this preprocessing when reading in multi files to clean up and only take what we need while reading to save time and mem\n",
    "def preprocessHRRRPrs(ds):\n",
    "    #make sure full grid is there\n",
    "    if ds.dims['x'] != 1799 and ds.dims['y'] != 1059:\n",
    "        print(\"bad dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    available_vars = [ v for v in hrrr_prs_vars if v in ds.keys() ]\n",
    "    available_pres = [ p for p in pres if p in ds.coords['isobaricInhPa'].values ]\n",
    "\n",
    "    if len(available_pres)<3:\n",
    "        return xr.Dataset(coords={'isobaricInhPa': ('isobaricInhPa', pres)})\n",
    "    #drop = [ d for d in dim if d not in ['lv_ISBL0', 'xgrid_0', 'ygrid_0'] ]\n",
    "\n",
    "    ds = ds[available_vars].sel(isobaricInhPa = available_pres)\n",
    "#     dim = ds.dims\n",
    "#     drop = [ d for d in dim if d not in ['isobaricInhPa', 'latitude', 'longitude'] ]\n",
    "    #ds = ds.drop(drop)\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def preprocessHRRRPrsALL(ds):\n",
    "            #make sure full grid is there\n",
    "    if ds.dims['x'] != 1799 and ds.dims['y'] != 1059:\n",
    "        print(\"bad dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    available_pres = [ p for p in pres if p in ds.coords['isobaricInhPa'].values ]\n",
    "\n",
    "    if len(available_pres)<3:\n",
    "        return xr.Dataset(coords={'isobaricInhPa': ('isobaricInhPa', pres)})\n",
    "\n",
    "    ds = ds.sel(isobaricInhPa = available_pres)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b1d08ee-db0e-4a6a-bed3-6f09c3af2f47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#call this preprocessing when reading in multi files to clean up and only take what we need while reading to save time and mem\n",
    "def preprocessHRRRSrf(ds):\n",
    "    #make sure full grid is there\n",
    "    if ds.dims['x'] != 1799 and ds.dims['y'] != 1059:\n",
    "        print(\"bad dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    available_vars = [ v for v in hrrr_sfc_vars if v in ds.keys() ]\n",
    "    ds = ds[available_vars]\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def preprocessHRRRSrfALL(ds):\n",
    "    #make sure full grid is there\n",
    "    if ds.dims['x'] != 1799 and ds.dims['y'] != 1059:\n",
    "        print(\"bad dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1b5103d-195f-4e43-9aef-7fc934eceaca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocessGFS(ds):\n",
    "    #get available vars\n",
    "    available_vars = [ v for v in gfs_vars if v in ds.keys() ]\n",
    "    #make sure full grid is there\n",
    "    if ds.dims['latitude'] != 361 and ds.dims['longitude'] != 720:\n",
    "        print(\"bad dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    ds = ds[available_vars]\n",
    "    return ds\n",
    "\n",
    "def preprocessGFSALL(ds):\n",
    "    #make sure full grid is there\n",
    "    if ds.dims['latitude'] != 361 and ds.dims['longitude'] != 720:\n",
    "        print(\"bad dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    else:\n",
    "        print(\"good dimensions in file: \",ds.encoding[\"source\"])\n",
    "        sys.stdout.flush()\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67dba438-2eea-4f38-8bd8-6646757eb7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_variables(ds, remove_vars):\n",
    "    for var in remove_vars:\n",
    "        keys = [v for v in ds.keys()]\n",
    "        if var in keys:\n",
    "            print(f'dropping {var} from dataset')\n",
    "            ds = ds.drop(var)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a918cd31-69e5-44c4-8a39-f96fde24cf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_in_one_file(fileList, model, prs):\n",
    "    dict_opts = [{'typeOfLevel': 'heightAboveGround','level':2}, \n",
    "            {'typeOfLevel': 'heightAboveGround','level':10},\n",
    "            {'typeOfLevel': 'surface','stepType': 'accum'}]\n",
    "    \n",
    "    if model == 'hrrr':\n",
    "        if prs:\n",
    "            return xr.open_dataset(fileList, \n",
    "                           engine='cfgrib',\n",
    "                           backend_kwargs={'indexpath':'','filter_by_keys':{'typeOfLevel': 'isobaricInhPa',}},\n",
    "                          )\n",
    "        else:\n",
    "\n",
    "            ds_save = []\n",
    "            for opt in dict_opts:\n",
    "                ds_save += [xr.open_dataset(fileList, \n",
    "                               engine='cfgrib',\n",
    "                               backend_kwargs={'indexpath':'','filter_by_keys':opt},\n",
    "                              )]\n",
    "            ds = xr.merge(ds_save, compat='override')\n",
    "            ds = drop_variables(ds, ['unknown','acpcp','sdwe','ssrun','bgrun'])\n",
    "\n",
    "            return ds\n",
    "\n",
    "    elif model == 'gfs':\n",
    "        ds_save = []\n",
    "        for opt in dict_opts:\n",
    "            ds_save += [xr.open_dataset(fileList, \n",
    "                           engine='cfgrib',\n",
    "                           backend_kwargs={'indexpath':'','filter_by_keys':opt},\n",
    "                          )]\n",
    "        ds = xr.merge(ds_save, compat='override')  \n",
    "        ds = drop_variables(ds, ['unknown','acpcp','sdwe','ssrun','bgrun'])\n",
    "\n",
    "        return ds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e09bdfb-0e3e-4a22-8417-da9a6fd93967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_data(fileList, model, prs):\n",
    "    \n",
    "    # list the datasets that you want extracted from grib file\n",
    "    dict_opts = [{'typeOfLevel': 'heightAboveGround','level':2}, \n",
    "                {'typeOfLevel': 'heightAboveGround','level':10},\n",
    "                {'typeOfLevel': 'surface','stepType': 'accum'},\n",
    "                {'typeOfLevel': 'meanSea'},\n",
    "                {'typeOfLevel':'surface', 'cfVarName': 'orog'}]\n",
    "  \n",
    "    if model == 'nam':\n",
    "        # this solution/option exists because open_mfdataset cannot handle the 'unknown' variables\n",
    "        # within the NAM grib files. Ideally would rather figure out how to use the usual open_mfdataset\n",
    "        # solution, but not able to at this time.\n",
    "        ds_opt_save = []\n",
    "        for opt in dict_opts:\n",
    "            ds_save = []\n",
    "            for file in fileList:\n",
    "                print(file)\n",
    "                ds_save += [xr.open_dataset(file, \n",
    "                               engine='cfgrib',\n",
    "                               backend_kwargs={'indexpath':'','filter_by_keys':opt},\n",
    "                              )]\n",
    "            ds_files = xr.combine_nested(ds_save, concat_dim='time') \n",
    "            ds_opt_save += [ds_files]\n",
    "\n",
    "        ds = xr.merge(ds_opt_save, compat='override') \n",
    "        ds = drop_variables(ds, ['unknown','acpcp','sdwe','ssrun','bgrun'])\n",
    "\n",
    "        return ds\n",
    "\n",
    "    elif model == 'hrrr':\n",
    "        if prs:\n",
    "            return xr.open_mfdataset(fileList, \n",
    "                            parallel=True,\n",
    "                           engine='cfgrib',\n",
    "                           concat_dim='time',\n",
    "                           combine='nested', \n",
    "                           backend_kwargs={'indexpath':'','filter_by_keys':{'typeOfLevel': 'isobaricInhPa',}},\n",
    "                           preprocess=preprocessHRRRPrsALL,\n",
    "                          )\n",
    "        else:\n",
    "\n",
    "            ds_save = []\n",
    "            for opt in dict_opts:\n",
    "                ds_save += [xr.open_mfdataset(fileList, \n",
    "                                parallel=True,\n",
    "                               engine='cfgrib',\n",
    "                               concat_dim='time',\n",
    "                               combine='nested', \n",
    "                               backend_kwargs={'indexpath':'','filter_by_keys':opt},\n",
    "                               preprocess=preprocessHRRRSrfALL,\n",
    "                              )]\n",
    "            ds = xr.merge(ds_save, compat='override')\n",
    "            ds = drop_variables(ds, ['unknown','acpcp','sdwe','ssrun','bgrun'])\n",
    "\n",
    "            return ds\n",
    "\n",
    "    elif model == 'gfs':\n",
    "        ds_save = []\n",
    "        for opt in dict_opts:\n",
    "            ds_save += [xr.open_mfdataset(fileList, \n",
    "                            parallel=True,\n",
    "                           engine='cfgrib',\n",
    "                           concat_dim='time',\n",
    "                           combine='nested', \n",
    "                           backend_kwargs={'indexpath':'','filter_by_keys':opt},\n",
    "                           preprocess=preprocessGFSALL,\n",
    "                          )]\n",
    "        ds = xr.merge(ds_save, compat='override')  \n",
    "        ds = drop_variables(ds, ['unknown','acpcp','sdwe','ssrun','bgrun'])\n",
    "\n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25ca95bf-d7ef-4e45-8432-34e96fcdab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ds_with_projection(ds):\n",
    "    # In order to slice by lat & lon values, need to transform the grid into a projection\n",
    "    # solution from https://stackoverflow.com/questions/58758480/xarray-select-nearest-lat-lon-with-multi-dimension-coordinates\n",
    "    projection = ccrs.LambertConformal(central_longitude=-97.5,\n",
    "                                 central_latitude=38.5,\n",
    "                                 standard_parallels=[38.5])\n",
    "    transform = np.vectorize(lambda x, y: projection.transform_point(x, y, ccrs.PlateCarree()))\n",
    "\n",
    "    # The grid should be aligned such that the projection x and y are the same\n",
    "    # at every y and x index respectively\n",
    "    grid_y = ds.isel(x=0)\n",
    "    grid_x = ds.isel(y=0)\n",
    "\n",
    "    _, proj_y = transform(grid_y.longitude, grid_y.latitude)\n",
    "    proj_x, _ = transform(grid_x.longitude, grid_x.latitude)\n",
    "\n",
    "    # ds.sel only works on the dimensions, so we can't just add\n",
    "    # proj_x and proj_y as additional coordinate variables\n",
    "    ds[\"x\"] = proj_x\n",
    "    ds[\"y\"] = proj_y\n",
    "    \n",
    "    # grab the unique latitude and longitude for NYSM sites\n",
    "    nysm_path = '/home/aevans/nysm/archive/nysm/netcdf/proc/2019/01/'\n",
    "    ds_nysm = xr.open_dataset(f'{nysm_path}20190101.nc')\n",
    "    df = ds_nysm.to_dataframe()\n",
    "\n",
    "    nysm_lats = df.lat.unique()\n",
    "    nysm_lons = df.lon.unique()\n",
    "\n",
    "    closest_to_nysm_lons_lats = [transform(nysm_lons[x], nysm_lats[x]) for x in range(len(nysm_lats))]\n",
    "    closest_to_nysm_lons = [closest_to_nysm_lons_lats[x][0] for x in range(len(nysm_lats))]\n",
    "    closest_to_nysm_lats = [closest_to_nysm_lons_lats[x][1] for x in range(len(nysm_lats))]\n",
    "\n",
    "    xx = xr.DataArray(closest_to_nysm_lons,dims='z')\n",
    "    yy = xr.DataArray(closest_to_nysm_lats,dims='z')\n",
    "    \n",
    "    return ds.sel(x=xx, y=yy, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "adb1e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_ds_with_projection_ok(ds):\n",
    "    # In order to slice by lat & lon values, need to transform the grid into a projection\n",
    "    # solution from https://stackoverflow.com/questions/58758480/xarray-select-nearest-lat-lon-with-multi-dimension-coordinates\n",
    "    projection = ccrs.LambertConformal(central_longitude=-98.8,\n",
    "                                 central_latitude=35.4,\n",
    "                                 standard_parallels=[35.4])\n",
    "    transform = np.vectorize(lambda x, y: projection.transform_point(x, y, ccrs.PlateCarree()))\n",
    "\n",
    "    # The grid should be aligned such that the projection x and y are the same\n",
    "    # at every y and x index respectively\n",
    "    grid_y = ds.isel(x=0)\n",
    "    grid_x = ds.isel(y=0)\n",
    "\n",
    "    _, proj_y = transform(grid_y.longitude, grid_y.latitude)\n",
    "    proj_x, _ = transform(grid_x.longitude, grid_x.latitude)\n",
    "\n",
    "    # ds.sel only works on the dimensions, so we can't just add\n",
    "    # proj_x and proj_y as additional coordinate variables\n",
    "    ds[\"x\"] = proj_x\n",
    "    ds[\"y\"] = proj_y\n",
    "    \n",
    "    # grab the unique latitude and longitude for NYSM sites\n",
    "    df = pd.read_csv('/home/aevans/landtype/geoinfo.csv')\n",
    "\n",
    "    oksm_lats = df.lat.unique()\n",
    "    oksm_lons = df.lon.unique()\n",
    "\n",
    "    closest_to_oksm_lons_lats = [transform(oksm_lons[x], oksm_lats[x]) for x in range(len(oksm_lats))]\n",
    "    closest_to_oksm_lons = [closest_to_oksm_lons_lats[x][0] for x in range(len(oksm_lats))]\n",
    "    closest_to_oksm_lats = [closest_to_oksm_lons_lats[x][1] for x in range(len(oksm_lats))]\n",
    "\n",
    "    xx = xr.DataArray(closest_to_oksm_lons,dims='z')\n",
    "    yy = xr.DataArray(closest_to_oksm_lats,dims='z')\n",
    "    \n",
    "    return ds.sel(x=xx, y=yy, method=\"nearest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "615d95ad-59ce-4283-a68c-5770eab41800",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_grid_bounds(ds, model, file):\n",
    "    ds = ds.assign_coords({\"longitude\": (((ds.longitude + 180) % 360) - 180)})\n",
    "        \n",
    "    if model!='gfs':\n",
    "        ds_grid = xr.open_dataset(file,engine='cfgrib',backend_kwargs={'indexpath':'', 'filter_by_keys':\n",
    "                                                           {'typeOfLevel': 'heightAboveGround','level':2,\n",
    "                                                           'cfVarName': 't2m'}})\n",
    "        central_longitude = (((ds_grid.t2m.attrs.get('GRIB_LoVInDegrees') + 180) % 360) - 180)\n",
    "        central_latitude = ds_grid.t2m.attrs.get('GRIB_LaDInDegrees')\n",
    "        projection = ccrs.LambertConformal(central_longitude= central_longitude,\n",
    "                             central_latitude=central_latitude,\n",
    "                             standard_parallels=[central_latitude])\n",
    "        transform = np.vectorize(lambda x, y: projection.transform_point(x, y, ccrs.PlateCarree()))\n",
    "\n",
    "        # The grid should be aligned such that the projection x and y are the same\n",
    "        # at every y and x index respectively\n",
    "        grid_y = ds.isel(x=0)\n",
    "        grid_x = ds.isel(y=0)\n",
    "        _, proj_y = transform(grid_y.longitude, grid_y.latitude)\n",
    "        proj_x, _ = transform(grid_x.longitude, grid_x.latitude)\n",
    "\n",
    "        # ds.sel only works on the dimensions, so we can't just add\n",
    "        # proj_x and proj_y as additional coordinate variables\n",
    "        ds[\"x\"] = proj_x\n",
    "        ds[\"y\"] = proj_y\n",
    "    \n",
    "    # set the longitude and latitude bounds of the smaller grid\n",
    "    # that you want to extract from the model data\n",
    "\n",
    "        # set the longitude and latitude bounds of the smaller grid\n",
    "    # that you want to extract from the model data\n",
    "    long_min, long_max = -103.5, -65\n",
    "    lat_min, lat_max = 33, 47\n",
    "    \n",
    "\n",
    "    if model!='gfs':\n",
    "        x_min, y_min = transform(long_min, lat_min)\n",
    "        x_max, y_max = transform(long_max, lat_max)\n",
    "        \n",
    "    # use the x, y min and max values from above to make the selection from the dataset\n",
    "    # this is better than solely selecting the point locations because I will need different solutions for diff models\n",
    "    # and those solutions should be independent of this cleaning script\n",
    "    \n",
    "    if model!='gfs':\n",
    "        ds_return = ds.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    else:\n",
    "        mask_lon = (ds.longitude >= long_min) & (ds.longitude <= long_max)\n",
    "        mask_lat = (ds.latitude >= lat_min) & (ds.latitude <= lat_max)\n",
    "        ds_return = ds.where(mask_lon & mask_lat, drop=True)\n",
    "        \n",
    "    return ds_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9efc307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_grid_bounds_ok(ds, model, file):\n",
    "    ds = ds.assign_coords({\"longitude\": (((ds.longitude + 180) % 360) - 180)})\n",
    "        \n",
    "    if model!='gfs':\n",
    "        ds_grid = xr.open_dataset(file,engine='cfgrib',backend_kwargs={'indexpath':'', 'filter_by_keys':\n",
    "                                                           {'typeOfLevel': 'heightAboveGround','level':2,\n",
    "                                                           'cfVarName': 't2m'}})\n",
    "        central_longitude = (((ds_grid.t2m.attrs.get('GRIB_LoVInDegrees') + 180) % 360) - 180)\n",
    "        central_latitude = ds_grid.t2m.attrs.get('GRIB_LaDInDegrees')\n",
    "        projection = ccrs.LambertConformal(central_longitude= central_longitude,\n",
    "                             central_latitude=central_latitude,\n",
    "                             standard_parallels=[central_latitude])\n",
    "        transform = np.vectorize(lambda x, y: projection.transform_point(x, y, ccrs.PlateCarree()))\n",
    "\n",
    "        # The grid should be aligned such that the projection x and y are the same\n",
    "        # at every y and x index respectively\n",
    "        grid_y = ds.isel(x=0)\n",
    "        grid_x = ds.isel(y=0)\n",
    "        _, proj_y = transform(grid_y.longitude, grid_y.latitude)\n",
    "        proj_x, _ = transform(grid_x.longitude, grid_x.latitude)\n",
    "\n",
    "        # ds.sel only works on the dimensions, so we can't just add\n",
    "        # proj_x and proj_y as additional coordinate variables\n",
    "        ds[\"x\"] = proj_x\n",
    "        ds[\"y\"] = proj_y\n",
    "    \n",
    "    # set the longitude and latitude bounds of the smaller grid\n",
    "    # that you want to extract from the model data\n",
    "    long_min, long_max = -103.5, -94.4\n",
    "    lat_min, lat_max = 33, 38\n",
    "    \n",
    "\n",
    "    if model!='gfs':\n",
    "        x_min, y_min = transform(long_min, lat_min)\n",
    "        x_max, y_max = transform(long_max, lat_max)\n",
    "        \n",
    "    # use the x, y min and max values from above to make the selection from the dataset\n",
    "    # this is better than solely selecting the point locations because I will need different solutions for diff models\n",
    "    # and those solutions should be independent of this cleaning script\n",
    "    \n",
    "    if model!='gfs':\n",
    "        ds_return = ds.sel(x=slice(x_min, x_max), y=slice(y_min, y_max))\n",
    "    else:\n",
    "        mask_lon = (ds.longitude >= long_min) & (ds.longitude <= long_max)\n",
    "        mask_lat = (ds.latitude >= lat_min) & (ds.latitude <= lat_max)\n",
    "        ds_return = ds.where(mask_lon & mask_lat, drop=True)\n",
    "        \n",
    "    return ds_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "78a0363a-b3b7-4dc3-8f40-8feee753fad3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#def main(model, year, init_time, prs=False, combined_file=False):\n",
    "def main(prs=False, combined_file=False):\n",
    "    '''\n",
    "    This is the main function that converts grib files from the GFS, NAM, and HRRR to parquet files.\n",
    "    The function loops over all months and day in a given year.\n",
    "    Within the conversion, specific variables are extracted. The datasets where these can be found\n",
    "    need to be specified within read_data(). Smaller forecast grids focused around NYS are defined \n",
    "    and saved within these parquet files.\n",
    "    \n",
    "    The following parameters need to be passed into main():\n",
    "    \n",
    "    model (str) - hrrr, nam, gfs\n",
    "    year (int) - the year of interest (e.g., 2020)\n",
    "    init_time (str) - initilization time for model, 00 or 12 UTC\n",
    "    prs (bool) - true if you want the pressure files, false if you only want surface\n",
    "    combined_file (bool) - this flag should be turned on if multiple forecast times exist in one grib file\n",
    "    '''\n",
    "    for year in [2018, 2019, 2020, 2021]:\n",
    "        model = 'gfs'\n",
    "        year = year \n",
    "        init_time = '00'\n",
    "\n",
    "        # input_path (str) - path to base location of data\n",
    "        # output_path (str) - where to write new smaller clean files\n",
    "        if combined_file:\n",
    "            input_path = '/home/aevans/ai2es/GFS/GFSv16_parallel'\n",
    "            output_path = '/home/aevans/ai2es/GFS/GFSv16_parallel/cleaned'\n",
    "        else:\n",
    "            input_path = f\"/home/aevans/ai2es/{model.upper()}/\"\n",
    "            output_path = f\"/home/aevans/ai2es/{model.upper()}/cleaned\"\n",
    "        # choosing to start at first ~forecast~ time rather than ~init~ time because of variable list inconsistencies\n",
    "        if model=='hrrr':\n",
    "            fh = range(1, 19) #forecast hours, second num exclusive\n",
    "        elif model=='nam':\n",
    "            fh = np.arange(1,37,1).tolist() + np.arange(39,85,3).tolist()\n",
    "        elif model =='gfs':\n",
    "            fh = np.arange(3, 99, 3)\n",
    "\n",
    "        #loop through months & days\n",
    "        for month in range(1, 13): #call all months in calendar year\n",
    "            num_days = calendar.monthrange(year, month)[1]\n",
    "            for day in range(1, num_days+1): #call all days in calendar month & respective year\n",
    "                print(\"This is your path!\")\n",
    "                print(\".    .\")\n",
    "                print(f'{output_path}{model.upper()}/{year}/{month}/{year}{month}{day}_{model}.t{init_time}z_fhAll.parquet')\n",
    "\n",
    "                start_date = datetime.datetime(year, month, day)\n",
    "                end_date = datetime.datetime(year, month, day)\n",
    "        \n",
    "                if combined_file:\n",
    "                    fileList = f'{input_path}{year}{str(month).zfill(2)}{str(day).zfill(2)}{init_time}_{model}.grb2'\n",
    "                else:\n",
    "                    fileList = days2files(input_path, start_date, end_date, init_time, model, fh, prs)\n",
    "                print(fileList)\n",
    "\n",
    "                if not fileList:\n",
    "                    print('No files exist to read!')\n",
    "                else:\n",
    "                    if combined_file:\n",
    "                        ds = read_data_in_one_file(fileList, model, prs)\n",
    "                        ds = define_grid_bounds(ds, model, fileList)\n",
    "                    else:\n",
    "                        ds = read_data(fileList, model, prs)\n",
    "                        ds = define_grid_bounds(ds, model, fileList[-1])\n",
    "                    \n",
    "                    #fill all na values with 0\n",
    "                    ds = ds.fillna(0)\n",
    "                    df = ds.to_dataframe(dim_order=None)\n",
    "\n",
    "                    if model == 'hrrr':\n",
    "                        new_index = ['time','y','x']\n",
    "                    elif model in ['gfs','nam']:\n",
    "                        new_index = ['time','latitude','longitude']\n",
    "\n",
    "                    # drop step since val time already has it and drop other data group names \n",
    "                    # as these do not include info that is necessary to keep\n",
    "                    if model=='gfs' and combined_file==True:\n",
    "                        df = df.reset_index().drop(['step', 'heightAboveGround', 'surface'], axis=1).set_index(new_index)\n",
    "                    else:\n",
    "                        df = df.reset_index().drop(['step', 'heightAboveGround', 'surface', 'meanSea'], axis=1).set_index(new_index)\n",
    "\n",
    "                    #save the data to parquet file\n",
    "                    sday = str(start_date.day).zfill(2)\n",
    "                    smonth = str(start_date.month).zfill(2)\n",
    "                    syear = start_date.year\n",
    "                    \n",
    "                    savepath = f'{output_path}{model.upper()}/{syear}/{smonth}/'\n",
    "                    #create this directory if it doesn't already exist\n",
    "                    Path(savepath).mkdir(parents=True, exist_ok=True)\n",
    "                    if model == 'hrrr':\n",
    "                        df.to_parquet(f'{savepath}{syear}{smonth}{sday}_{model}.t{init_time}z_wrfsfc_fhAll.parquet')\n",
    "                    else:\n",
    "                        df.to_parquet(f'{savepath}{syear}{smonth}{sday}_{model}.t{init_time}z_fhAll.parquet')\n",
    "                        \n",
    "            print(\"This is your path!\")\n",
    "            print(\".    .\")\n",
    "            print(f'{output_path}{model.upper()}/{year}/{month}/{year}{month}{day}_{model}.t{init_time}z_wrfsfc_fhAll.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b59861bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     pool = multiprocessing.Pool(16)\n",
    "#     pool.map(main, [])\n",
    "#     pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a8ab04c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is your path!\n",
      ".    .\n",
      "/home/aevans/ai2es/GFS/cleanedGFS/2018/1/201811_gfs.t00z_wrfsfc_fhAll.parquet\n",
      "['/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_009.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_036.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_045.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_090.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2']\n",
      "['/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_009.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_036.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_045.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_090.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2', '/home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2']\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_045.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_090.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_036.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_009.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_036.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_045.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_009.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_090.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_039.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_021.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_030.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_066.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_075.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_006.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_042.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_060.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_081.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_003.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_072.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_093.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_012.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_033.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_087.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_018.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_096.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_027.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_051.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_024.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_015.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_063.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_048.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_084.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_054.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_078.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_069.grb2\n",
      "good dimensions in file:  /home/aevans/ai2es/GFS/2018/01/gfs_4_20180101_0000_057.grb2\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4539554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "main('nam', 2018, '00')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "44818f36aeaf89db1a1d21a2bee6031a28b4e41595a65903b38b9b0c4417365f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
